input {
    jdbc {
        jdbc_connection_string => "jdbc:postgresql://postgres:5432/postgres"
        jdbc_user => "postgres"
        jdbc_password => "${PG_PASSWORD}"
        jdbc_driver_class => "org.postgresql.Driver"
        schedule => "0 * * * *" # cronjob schedule format (see "Helpful Links")
        statement => "SELECT * FROM public.{pg_table}" # the PG command for retrieving the documents IMPORTANT: no semicolon!
        jdbc_paging_enabled => "true"
        jdbc_page_size => "300"
    }
}

filter {
    mutate {
        # list of fields to remove from the pg input (fields as string)
        remove_field => ["{pg_table_column_1}", "{pg_table_column_2}"]
        # substitutes (replaces) parts of the value in {pg_table_column_i}
        # that matches {regex_i} with {value_i}
        gsub => [
            "{pg_table_column_3}", "{regex_1}", "{value_1}",
            "{pg_table_column_4}", "{regex_2}", "{value_2}"
        ]
    }
    # this part is used to parse jsonb type of values
    mutate {
        join => { "{pg_table_column_5}" => "," }
        replace => { "{pg_table_column_5}" => "[%{{pg_table_column_5}}]" }
    }
    json {
        source => "{pg_table_column_5}"
        target => "{pg_table_column_5}"
    }
}

output {
    # used to output the values in the terminal (DEBUGGING)
    # once everything is working, comment out this line
    stdout { codec => "json" }
    # used to output the values into elasticsearch
    elasticsearch {
        hosts => ["elasticsearch"]
        index => "{es-index}"
        document_id => "document_%{pg_table_column_id}"
        doc_as_upsert => true # upserts documents (e.g. if the document does not exist, creates a new record)
    }
}